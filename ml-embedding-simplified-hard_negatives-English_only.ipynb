{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding training and testing data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "import ipdb\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "my_stderr = sys.stderr = open('errors.txt', 'w')  # redirect stderr to file\n",
    "get_ipython().log.handlers[0].stream = my_stderr  # log errors to new stderr\n",
    "get_ipython().log.setLevel(logging.INFO)  # errors are logged at info level\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import ipdb\n",
    "from tqdm import tqdm\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LASER=\"LASER-master\"\n",
    "mldir = \"ml-training/SECRECY-EN/data_hard_negatives\" # training data and embeddings are held here\n",
    "model_name = \"SECRECY-EN\"\n",
    "edir = mldir + \"/embed_english\" # location of laser script that that embeds to LASER SS\n",
    "#embedding_name = 'USE'\n",
    "embedding_name = 'USEm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def USE(txt_list, use_embedder):\n",
    "    output = []\n",
    "    txt_df = pd.Series(txt_list)\n",
    "    batch_size = 12\n",
    "    for k,g in tqdm(txt_df.groupby(np.arange(len(txt_df))//batch_size)):\n",
    "        output.append(use_embedder(g.tolist()).numpy())\n",
    "    output = np.concatenate(output, axis=0).flatten()\n",
    "    #print(output.shape)\n",
    "    #print(output.min())\n",
    "    #print(output.max())\n",
    "    #print('\\n')\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed train and dev sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing:\n",
      "Starting train\n",
      "Copying txt/lbl files\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(edir):\n",
    "    os.mkdir(edir)\n",
    "\n",
    "#lang=\"en\"\n",
    "for lang in ['en']:\n",
    "    print('\\nProcessing:')\n",
    "    embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-large/5\")\n",
    "    #embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3\")\n",
    "\n",
    "    for part in ('train', 'dev'):\n",
    "        print('Starting ' + part)\n",
    "        # Spanish only exists for test\n",
    "        if (lang=='es') & (part!='test'):\n",
    "            continue\n",
    "        cfname = os.path.join(mldir, part)\n",
    "        if os.path.exists(cfname + '.' + embedding_name + '.enc.' + lang): # embedding_name\n",
    "            print(cfname + '.' + embedding_name + '.enc.' + lang + ' Exists')\n",
    "        txt = []\n",
    "        \n",
    "        # copy original files to embed directory, then embed it\n",
    "        print('Copying txt/lbl files')\n",
    "        os.system('cp ' + cfname + '.txt.' + lang + ' ' + os.path.join(edir, part) + '.txt.' + lang)\n",
    "        os.system('cp ' + cfname + '.lbl.' + lang + ' ' + os.path.join(edir, part) + '.lbl.' + lang)\n",
    "        \n",
    "        # embed file in embed directory\n",
    "        with open(cfname + '.txt.' + lang) as f:\n",
    "            for l in f:\n",
    "                txt.append(l)\n",
    "        use_np = USE(txt, embed)\n",
    "        print(use_np.dtype, use_np.shape)\n",
    "        use_np.tofile(edir + \"/\" + part + '.' + embedding_name + '.enc.' + lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test, split into smaller files\n",
    "# split -l 1000 -d -a 3 test.txt.en test.txt.en_\n",
    "# mkdir test_txt_en\n",
    "# mv test.txt.en_* test_txt_en/\n",
    "\n",
    "# split -l 1000 -d -a 3 test.txt.es test.txt.es_\n",
    "# mkdir test_txt_es\n",
    "# mv test.txt.es_* test_txt_es/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(edir):\n",
    "    os.mkdir(edir)\n",
    "\n",
    "#lang=\"en\"\n",
    "for lang in ['en']:\n",
    "    print('\\nProcessing:')\n",
    "    embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-large/5\")\n",
    "    #embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3\")\n",
    "\n",
    "    for part in ['test']:\n",
    "        cfname = mldir + '/test_txt_en/test.txt.en_'\n",
    "        \n",
    "        # copy original files to embed directory, then embed it\n",
    "        print('Copying txt/lbl files')\n",
    "        os.system('cp ' + mldir + '/test.txt.en' + ' ' + os.path.join(edir, part) + '.txt.' + lang)\n",
    "        os.system('cp ' + mldir + '/test.lbl.en' + ' ' + os.path.join(edir, part) + '.lbl.' + lang)\n",
    "        \n",
    "        # embed file in embed directory\n",
    "        for i in range(692):\n",
    "            txt = []\n",
    "            if i>=100:\n",
    "                filename = cfname + str(i)\n",
    "            if (i>=10) & (i<=99):\n",
    "                filename = cfname + '0' + str(i)\n",
    "            if i<=9:\n",
    "                filename = cfname + '00' + str(i)\n",
    "            print('Reading from: ' + filename)\n",
    "            with open(filename) as f:\n",
    "                for l in f:\n",
    "                    txt.append(l)\n",
    "            use_np = USE(txt, embed)\n",
    "            #print(use_np.dtype, use_np.shape)\n",
    "            print('Writing to: ' + edir + \"/\" + part + '.' + embedding_name + '.enc.' + lang + '_' + str(i))\n",
    "            print('\\n')\n",
    "            use_np.tofile(edir + \"/\" + part + '.' + embedding_name + '.enc.' + lang + '_' + str(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
